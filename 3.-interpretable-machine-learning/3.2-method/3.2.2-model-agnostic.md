# 3.2.2 Model-Agnostic

## 3.2.2.1 Variable Importance

Variable Importance represents the statistical significance of each variable in the data with respect to its effect on the generated model. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature.

We called varimp\_plot\(\) API of H2O to output a bar type plot, this plot shows the top ten most important features in turn for the model. The degree of each feature’s importance is the length of the bar after it has been scaled between 0 and 1.  

![](../../.gitbook/assets/image%20%289%29.png)

The calculation of variable importance for the tree-based model is determined by calculating the relative influence of each variable: whether that variable was selected to split on during the tree building process, and how much the squared error \(overall trees\) improved \(decreased\) as a result. 

![Squared Error Equation](../../.gitbook/assets/image%20%286%29.png)

And for the non-tree-based model, we could fit the model using the standardized independent variables and compare the standardized coefficients as the variable importance.

## 3.2.2.2 Partial Dependence Plot \(PDP\)

The partial dependence plot \(short PDP or PD plot\) shows the marginal effect one or two features have on the predicted outcome of a machine learning model. A partial dependence plot can show the relationship between the target and a feature. The effect of a variable is measured in change in the mean response.

![Example of PDP](../../.gitbook/assets/image%20%281%29.png)

  
The partial dependence of the response ƒ at a point Xs is defined as:

![](../../.gitbook/assets/image%20%287%29.png)

## 3.2.2.3 Individual Conditional Expectation \(ICE\)

The difference with the partial dependence plot \(PDP\) is that the Individual Conditional Expectation \(ICE\) plots to display one line per instance that shows how the instance's prediction changes when a feature changes. An ICE plot visualizes the dependence of the prediction on a feature for each instance separately, resulting in one line per instance, compared to one line overall in partial dependence plots. ACE plots can help us discover the heteroscedasticity from the dataset which is hard to find out only through partial dependence plots.

![](../../.gitbook/assets/image%20%288%29.png)

