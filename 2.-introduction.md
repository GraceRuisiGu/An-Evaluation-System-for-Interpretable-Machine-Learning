---
description: Introduction to Interpretable Models and Evaluation Systems
---

# 2. Introduction

## 2.1 Interpretable Machine Learning Model

On its own, interpretability is a broad, poorly defined concept. Taken to its full generality, to interpret data means to extract information \(of some form\) from them. The set of methods falling under this umbrella spans everything from designing an initial experiment to visualizing final results.

We define interpretable machine learning as the extraction of relevant knowledge from a machine-learning model concerning relationships either contained in data or learned by the model. Here, we view knowledge as being relevant if it provides insight for a particular audience into a chosen problem.

We referred to a [book](https://christophm.github.io/interpretable-ml-book/simple.html) that helps us understand the Interpretable Machine learning Model. The methods we use here are machine learning models and model-agnostic. Here, machine learning models will be the algorithms that help us generate the models.

Ex: **GBM, GLM, DRF, XGBoost, etc**. While the model-agnostic will be the methods that are used to interpret these models, ex: **Variable importance, Partial Dependacy Plot, ICE plot, etc**.

## 2.2 Evaluation System of Interpretable Machine Learning Model

There is no real consensus about what interpretability is in machine learning. Nor is it clear how to measure it. But there is some initial research on this and an attempt to formulate some approaches for evaluation, as described in the following section.

**Application-level evaluation \(real task\)**: Put the explanation into the product and have it tested by the end-user. Imagine fracture detection software with a machine learning component that locates and marks fractures in X-rays. At the application level, radiologists would test the fracture detection software directly to evaluate the model. This requires a good experimental setup and an understanding of how to assess quality. A good baseline for this is always how good a human would be at explaining the same decision.

**Human-level evaluation \(simple task\)** is a simplified application-level evaluation. The difference is that these experiments are not carried out with the domain experts, but with laypersons. This makes experiments cheaper \(especially if the domain experts are radiologists\) and it is easier to find more testers. An example would be to show a user different explanations and the user would choose the best one.

**Function level evaluation \(proxy task\)** does not require humans. This works best when the class of model used has already been evaluated by someone else in a human level evaluation. For example, it might be known that the end-users understand decision trees. In this case, a proxy for explanation quality may be the depth of the tree. Shorter trees would get a better explainability score. It would make sense to add the constraint that the predictive performance of the tree remains good and does not decrease too much compared to a larger tree.

The following chapter focuses on what are the different interpretable models and the methods to evaluate them for explanations on individual predictions on the function level. This is the current research we are dealing with to understand and propose some standards.

## 2.3 Customized the Interpretable Model with Specific Focus

This section will be clearly explaining, the user selection of the evaluation system that we are structuring. We consider an example, trying to run it programmatically in a Notebook and come to a conclusion explaining the metric and the score of the metric, based on the User selection use case, and demonstrate how the scoring system would work for the structure that we have formalized.

We explain - why and how a user would select the metric based on the score, and provide a suggestion list, and demonstrate it even further with the help of a Jupyter Notebook as an example, and explain our thoughts on the results with an explanation.

The radar chart, is the t 

 We fo showcase the inally, discuss over this at the end as a closure, and put 

