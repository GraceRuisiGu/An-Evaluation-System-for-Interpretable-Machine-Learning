---
description: Introduction to Interpretable Models and Evaluation Systems
---

# 2. Introduction

## 2.1 Interpretable Machine Learning Model

We referred a [book](https://christophm.github.io/interpretable-ml-book/evaluation-of-interpretability.html) that helps us understand Doshi-Velez and Kim \(2017\) proposal of three main levels for the evaluation of interpretability:


## 2.2 Evaluation System of Interpretable Machine Learning Model

There is no real consensus about what interpretability is in machine learning. Nor is it clear how to measure it. But there is some initial research on this and an attempt to formulate some approaches for evaluation, as described in the following section.

**Application-level evaluation \(real task\)**: Put the explanation into the product and have it tested by the end-user. Imagine fracture detection software with a machine learning component that locates and marks fractures in X-rays. At the application level, radiologists would test the fracture detection software directly to evaluate the model. This requires a good experimental setup and an understanding of how to assess quality. A good baseline for this is always how good a human would be at explaining the same decision.

**Human-level evaluation \(simple task\)** is a simplified application-level evaluation. The difference is that these experiments are not carried out with the domain experts, but with laypersons. This makes experiments cheaper \(especially if the domain experts are radiologists\) and it is easier to find more testers. An example would be to show a user different explanations and the user would choose the best one.

**Function level evaluation \(proxy task\)** does not require humans. This works best when the class of model used has already been evaluated by someone else in a human level evaluation. For example, it might be known that the end-users understand decision trees. In this case, a proxy for explanation quality may be the depth of the tree. Shorter trees would get a better explainability score. It would make sense to add the constraint that the predictive performance of the tree remains good and does not decrease too much compared to a larger tree.

The following chapter focuses on what are the different interpretable models and the methods to evaluate them for explanations on individual predictions on the function level. This is the current research we are dealing with to understand and propose some standards.

## 2.3 Customized the Interpretable Model with Specific Focus 

