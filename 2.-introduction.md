---
description: Introduction to Interpretable Models and Evaluation Systems
---

# 2. Introduction

## 2.1 What does Model Interpretability mean?

There is no mathematical definition of interpretability. 

A \(non-mathematical\) definition is: **Interpretability is the degree to which a human can understand the cause of a decision.** 

Another one is: **Interpretability is the degree to which a human can consistently predict the model's result**. 

The higher the interpretability of a machine learning model, the easier it is for someone to comprehend why certain decisions or predictions have been made. A model is better interpretable than another model if its decisions are easier for a human to comprehend than decisions from the other model.

## 2.2 What is Evaluations System?

There is no real consensus about what interpretability is in machine learning. Nor is it clear how to measure it. But there is some initial research on this and an attempt to formulate some approaches for evaluation, as described in the following section.

We refered a [book](https://christophm.github.io/interpretable-ml-book/evaluation-of-interpretability.html) that helps us understand Doshi-Velez and Kim \(2017\) proposal of three main levels for the evaluation of interpretability:

**Application level evaluation \(real task\)**: Put the explanation into the product and have it tested by the end user. Imagine fracture detection software with a machine learning component that locates and marks fractures in X-rays. At the application level, radiologists would test the fracture detection software directly to evaluate the model. This requires a good experimental setup and an understanding of how to assess quality. A good baseline for this is always how good a human would be at explaining the same decision.

**Human level evaluation \(simple task\)** is a simplified application level evaluation. The difference is that these experiments are not carried out with the domain experts, but with laypersons. This makes experiments cheaper \(especially if the domain experts are radiologists\) and it is easier to find more testers. An example would be to show a user different explanations and the user would choose the best one.

**Function level evaluation \(proxy task\)** does not require humans. This works best when the class of model used has already been evaluated by someone else in a human level evaluation. For example, it might be known that the end users understand decision trees. In this case, a proxy for explanation quality may be the depth of the tree. Shorter trees would get a better explainability score. It would make sense to add the constraint that the predictive performance of the tree remains good and does not decrease too much compared to a larger tree.

The following chapter focuses on the what are the different interpretable models and the methods to evaluat them for explanations on individual predictions on the function level. This is the current researcc we are dealing with to understand and propose some standards.

